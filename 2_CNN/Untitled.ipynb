{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b536d5",
   "metadata": {},
   "source": [
    "# Convolution Neural Network - Example of Cats and Dog Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfea3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# If we want to read the data from folders - ImageDataGenerator from Keras will help us to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "405504ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensorflow version\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da29f12",
   "metadata": {},
   "source": [
    "# Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7370081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the training set\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1.0/255,  # Normalizing the data. Bringing the value between 0 and 1.\n",
    "                                                       # We are dividing by 255 because, the image have the color range\n",
    "                                                       # from 0 to 255. So, dividing each pixel of an image by 255 will \n",
    "                                                       # bring the value between 0 and 1 \n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "# ImageDataGenerator will also allow us to do feature scaling on images. So that we will have some more new images. \n",
    "# This is called Data Augmentation.\n",
    "\n",
    "# Note: \n",
    "# Only rescaling operation, we need to apply to our test dataset. Remaining things mentioned in the above no need to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65c16f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2002 images belonging to 2 classes.\n",
      "Found 2002 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Need to read the data from the folder\n",
    "\n",
    "# Preprocessing the training set\n",
    "train_set = train_datagen.flow_from_directory('Datasets/train',\n",
    "                                               target_size =  (64,64), # image size\n",
    "                                               batch_size = 32,   # In each epoch 32 iterations will be done\n",
    "                                               class_mode = 'binary') # We are doing class and Dog i.e, 0 or 1.\n",
    "                                                # If we dont have binary classification problem, in that case we specify\n",
    "                                                # class_mode = 'categorical'\n",
    "\n",
    "# Preprocessing the test set\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255)\n",
    "test_set = test_datagen.flow_from_directory('Datasets/train',\n",
    "                                             target_size =  (64,64),\n",
    "                                             batch_size = 32,\n",
    "                                             class_mode = 'binary')\n",
    "\n",
    "\n",
    "# The Target size should be same, when constructing the Neural Network.\n",
    "# Note:\n",
    "# If the image size is bigger we can make it as a smaller image, by specifying in the target_size\n",
    "# If the image size is smaller, we can make it as a bigger image. But, we wont get clarity. so avoid giving target_size bigger \n",
    "# value compared to actual size of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ff8d3",
   "metadata": {},
   "source": [
    "# Part 2 - Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b245df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intializing the Sequenatial model. - Which will help us to do forward and backward propogation\n",
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "# Step 1 - Convolution layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, padding='same', kernel_size=3, activation='relu', input_shape=[64,64,3]))\n",
    "# For each of pur 32 filter kernel size is 3x3.\n",
    "# Input size is 64x64 as we mentioned in train_Set and 3 is RGB image\n",
    "# padding='same'- After every filter operation, it will ensure that output image size is same.\n",
    "# If we not specify strides, then it will taken as 1.\n",
    "\n",
    "# Step 2 - Pooling\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Adding second convolution layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, padding='same', kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Step 4 - Full Connection\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "# Units is number of hidden neurons.\n",
    "# We can any number of dense layer\n",
    "\n",
    "# Step 5 - Output Layer\n",
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "# Since it is a binary classification, we using sigmoid as an activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad5146fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               1048704   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,058,977\n",
      "Trainable params: 1,058,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary of our CNN  model\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbca906",
   "metadata": {},
   "source": [
    "# Part3 - Training our CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f90da584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "63/63 [==============================] - 19s 307ms/step - loss: 0.6152 - accuracy: 0.6688 - val_loss: 0.5956 - val_accuracy: 0.6843\n",
      "Epoch 2/25\n",
      "63/63 [==============================] - 20s 314ms/step - loss: 0.5943 - accuracy: 0.6893 - val_loss: 0.5729 - val_accuracy: 0.7018\n",
      "Epoch 3/25\n",
      "63/63 [==============================] - 20s 313ms/step - loss: 0.5692 - accuracy: 0.7103 - val_loss: 0.5531 - val_accuracy: 0.7198\n",
      "Epoch 4/25\n",
      "63/63 [==============================] - 19s 309ms/step - loss: 0.5485 - accuracy: 0.7173 - val_loss: 0.5404 - val_accuracy: 0.7123\n",
      "Epoch 5/25\n",
      "63/63 [==============================] - 19s 304ms/step - loss: 0.5274 - accuracy: 0.7473 - val_loss: 0.5370 - val_accuracy: 0.7153\n",
      "Epoch 6/25\n",
      "63/63 [==============================] - 20s 315ms/step - loss: 0.5173 - accuracy: 0.7547 - val_loss: 0.5873 - val_accuracy: 0.6848\n",
      "Epoch 7/25\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.5082 - accuracy: 0.7502 - val_loss: 0.4507 - val_accuracy: 0.7977\n",
      "Epoch 8/25\n",
      "63/63 [==============================] - 19s 294ms/step - loss: 0.4818 - accuracy: 0.7757 - val_loss: 0.4402 - val_accuracy: 0.7942\n",
      "Epoch 9/25\n",
      "63/63 [==============================] - 20s 320ms/step - loss: 0.4792 - accuracy: 0.7707 - val_loss: 0.4206 - val_accuracy: 0.8042\n",
      "Epoch 10/25\n",
      "63/63 [==============================] - 27s 428ms/step - loss: 0.4645 - accuracy: 0.7867 - val_loss: 0.4578 - val_accuracy: 0.7872\n",
      "Epoch 11/25\n",
      "63/63 [==============================] - 20s 322ms/step - loss: 0.4567 - accuracy: 0.7762 - val_loss: 0.4518 - val_accuracy: 0.7807\n",
      "Epoch 12/25\n",
      "63/63 [==============================] - 24s 383ms/step - loss: 0.4511 - accuracy: 0.7942 - val_loss: 0.4050 - val_accuracy: 0.8162\n",
      "Epoch 13/25\n",
      "63/63 [==============================] - 22s 350ms/step - loss: 0.4314 - accuracy: 0.7987 - val_loss: 0.4003 - val_accuracy: 0.8142\n",
      "Epoch 14/25\n",
      "63/63 [==============================] - 20s 313ms/step - loss: 0.4363 - accuracy: 0.8077 - val_loss: 0.3719 - val_accuracy: 0.8382\n",
      "Epoch 15/25\n",
      "63/63 [==============================] - 22s 345ms/step - loss: 0.4190 - accuracy: 0.8172 - val_loss: 0.3719 - val_accuracy: 0.8252\n",
      "Epoch 16/25\n",
      "63/63 [==============================] - 24s 384ms/step - loss: 0.4067 - accuracy: 0.8117 - val_loss: 0.3625 - val_accuracy: 0.8362\n",
      "Epoch 17/25\n",
      "63/63 [==============================] - 21s 327ms/step - loss: 0.4043 - accuracy: 0.8242 - val_loss: 0.3674 - val_accuracy: 0.8312\n",
      "Epoch 18/25\n",
      "63/63 [==============================] - 28s 440ms/step - loss: 0.3866 - accuracy: 0.8262 - val_loss: 0.3308 - val_accuracy: 0.8611\n",
      "Epoch 19/25\n",
      "63/63 [==============================] - 25s 402ms/step - loss: 0.3825 - accuracy: 0.8337 - val_loss: 0.3508 - val_accuracy: 0.8417\n",
      "Epoch 20/25\n",
      "63/63 [==============================] - 26s 410ms/step - loss: 0.3928 - accuracy: 0.8197 - val_loss: 0.3210 - val_accuracy: 0.8621\n",
      "Epoch 21/25\n",
      "63/63 [==============================] - 26s 415ms/step - loss: 0.3792 - accuracy: 0.8407 - val_loss: 0.3370 - val_accuracy: 0.8511\n",
      "Epoch 22/25\n",
      "63/63 [==============================] - 26s 415ms/step - loss: 0.3799 - accuracy: 0.8192 - val_loss: 0.3074 - val_accuracy: 0.8631\n",
      "Epoch 23/25\n",
      "63/63 [==============================] - 26s 419ms/step - loss: 0.3548 - accuracy: 0.8487 - val_loss: 0.2889 - val_accuracy: 0.8801\n",
      "Epoch 24/25\n",
      "63/63 [==============================] - 25s 400ms/step - loss: 0.3641 - accuracy: 0.8372 - val_loss: 0.2967 - val_accuracy: 0.8756\n",
      "Epoch 25/25\n",
      "63/63 [==============================] - 26s 419ms/step - loss: 0.3487 - accuracy: 0.8462 - val_loss: 0.2940 - val_accuracy: 0.8771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2121b03f908>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the CNN\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model on the Training set and evaluating on the test set\n",
    "cnn.fit(x= train_set, validation_data = test_set, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff656ab",
   "metadata": {},
   "source": [
    "# Part4 - Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3066a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save('dog_cat_classification_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a357b099",
   "metadata": {},
   "source": [
    "# Part5 - Loading the save model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "072a6ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('dog_cat_classification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db655c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9766883]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions from the load model\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "test_image = image.load_img('Datasets/test/Dog/10003.jpg', target_size=[64,64,3])\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = test_image/255\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = model.predict(test_image)\n",
    "\n",
    "result\n",
    "\n",
    "# Steps:\n",
    "# Loading the image and give the targetsize same as we mentioned in the train set and test set\n",
    "# Conveting it to an array\n",
    "# Dividing by 255 because we rescaling in the test set\n",
    "# expand the dims\n",
    "# predict the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42615a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is a Dog\n"
     ]
    }
   ],
   "source": [
    "if result[0] <= 0.5:\n",
    "    print(\"The image is a Cat\")\n",
    "else:\n",
    "    print(\"The image is a Dog\")\n",
    "    \n",
    "# We using [0] for Cat because, in Dataset folder, Cat folder will be first and Dog folder will be second \n",
    "# because of alphabetical order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1256af9",
   "metadata": {},
   "source": [
    "# Issues while Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "383db3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you face any error like below:\n",
    "# PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x000002121AC9D8E0>\n",
    "\n",
    "# This issue is due to: There was an issue with one of the img that was causing an issue\n",
    "# To solve the above issue, run the below command and find the incorrect image \n",
    "# and replace that image with the new image with the same label name as mentioned previously in that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcdfd19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from pathlib import Path\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "path = Path(\"Datasets/train/Cat\").rglob(\"*.jpg\")\n",
    "for img_p in path:\n",
    "    try:\n",
    "        img = PIL.Image.open(img_p)\n",
    "    except PIL.UnidentifiedImageError:\n",
    "            print(img_p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
